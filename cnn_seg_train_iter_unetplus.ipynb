{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805b4878-fed6-4dea-af0b-bbf21a903c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#os.environ['LD_LIBRARY_PATH']= '$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64'\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6450ba-f2a8-407b-b4b3-a48d87b00571",
   "metadata": {},
   "source": [
    "### Prep lists of input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0d91b2-cef1-4c19-b08c-bdc1f682d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from random import sample, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab307572-c831-44de-a43a-d7d872a397d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_list = ['resnet34']\n",
    "avail_suffix = ['rgb', 'tpi', 'shade', 'ndvi', 'dsm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40650c2d-2675-4431-a3d3-a7124ee8ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir('/project/cper_neon_aop/cper_pdog_uas')\n",
    "\n",
    "# set directories for training data and labels\n",
    "DATA_FOLDER = './cnn_train_images/{}_{}.tif'\n",
    "LABEL_FOLDER = './cnn_train_labels/{}_labels.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964c07d0-dd79-486d-9a45-26cf7b84d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csvs with training information\n",
    "df_tiles = pd.read_csv('train_tiles/train_bboxes_all_assigned.csv')\n",
    "df_polys = pd.read_csv('train_polys/train_polys_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a92bddfb-90ba-468b-a358-9ee021cb0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all ids to be used\n",
    "label_files = glob(LABEL_FOLDER.replace('{}', '*'))\n",
    "all_ids = [re.sub('_labels.tif', '', os.path.basename(f)) for f in label_files]\n",
    "all_tiles = list(set(['_'.join(y.split('_')[2:]) for y in all_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09eb2d92-e56e-431a-8b11-2d599336ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate training and test data and get paths to files\n",
    "all_files = glob(DATA_FOLDER.replace('{}', '*'))\n",
    "all_train_tiles = [x for x in df_tiles.apply(lambda x: '_'.join([x.Pasture, x.Tile]) if x.Train == 1 else '', axis=1) if x != '' and x in all_tiles]\n",
    "test_tiles = list(set(all_tiles) - set(all_train_tiles))\n",
    "\n",
    "all_train_ids = [x for x in all_ids if '_'.join(x.split('_')[-3:]) in all_train_tiles]\n",
    "test_ids = list(set(all_ids) - set(all_train_ids))\n",
    "\n",
    "seed(321)\n",
    "valid_ids = sample(all_train_ids, int(np.ceil(len(all_train_ids)*0.3)))\n",
    "train_ids = list(set(all_train_ids) - set(valid_ids))\n",
    "\n",
    "train_files = [f for f in all_files if '_'.join(os.path.basename(f).split('_')[:-1]) in train_ids]\n",
    "valid_files = [f for f in all_files if '_'.join(os.path.basename(f).split('_')[:-1]) in valid_ids]\n",
    "test_files = [f for f in all_files if '_'.join(os.path.basename(f).split('_')[:-1]) in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c06f11-1701-4cc8-81be-d7c26c7a0ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_ids = df_tiles[(df_tiles['trainer'] != 'Nick') &\n",
    "                    (df_tiles['Digitize'] == 1)].apply(lambda x: '_'.join([x.Pasture, x.Tile]), axis=1)\n",
    "#all_tiles#\n",
    "[x for x in all_tiles if x not in tile_ids.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ace3e5-8ab0-4e18-8ea6-b3d00c690c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in tile_ids.to_list() if x not in all_tiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180703c2-6582-4b02-8ef2-78e9de7db704",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "Writing helper class for data extraction, tranformation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "100b2c6e-9f49-42d8-9346-4000ffc9c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "616adccf-9bcb-421d-9bb6-04d16c48815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        ids (list): list of unique ids for all images\n",
    "        images_path (str): path to data images\n",
    "        masks_path (str): path to label masks\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    CLASSES = ['other', 'burrow']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            ids,\n",
    "            suffix_list,\n",
    "            images_path,\n",
    "            masks_path, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            suffix_dict = {\n",
    "        'rgb': {'channels': 3,\n",
    "                'dtype': 'uint8'},\n",
    "        'tpi': {'channels': 1,\n",
    "                'dtype': 'float32'},\n",
    "        'dsm': {'channels': 1,\n",
    "                'dtype': 'float32'},\n",
    "        'shade': {'channels': 1,\n",
    "                  'dtype': 'float32'},\n",
    "        'ndvi': {'channels': 1,\n",
    "                  'dtype': 'float32'}\n",
    "    }\n",
    "    ):\n",
    "        # get IDs as attribute\n",
    "        self.ids = ids\n",
    "        \n",
    "        # get suffix info\n",
    "        self.suffix_dict = suffix_dict\n",
    "        \n",
    "        # get list of suffixes as attribute\n",
    "        self.suffix_list = suffix_list\n",
    "        \n",
    "        # List of files\n",
    "        self.images_fps = []\n",
    "        self.masks_fps = [masks_path.format(id) for id in ids]\n",
    "        for id in ids:\n",
    "            self.images_fps.append({s: images_path.format(id, s) for s in suffix_list})\n",
    "            \n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image_list = []\n",
    "        self.image_dict = {}\n",
    "        for s in self.suffix_list:\n",
    "            image_s = np.asarray(io.imread(self.images_fps[i][s]), dtype=self.suffix_dict[s]['dtype'])\n",
    "            if len(image_s.shape) == 2:\n",
    "                image_s = np.expand_dims(image_s, axis=-1)\n",
    "            image_s[np.isnan(image_s)] = 0\n",
    "            image_list.append(image_s)\n",
    "            self.image_dict[s] = image_s\n",
    "        if len(image_list) == 1:\n",
    "            image = image_list[0]\n",
    "        else:\n",
    "            image = np.concatenate(image_list, axis=-1)\n",
    "        mask = np.asarray(io.imread(self.masks_fps[i]), dtype='float32')\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1)#.astype('float32')\n",
    "        #print('fetched: ', self.ids[i])\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        else:\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1).astype('float32'))\n",
    "            mask = torch.from_numpy(mask.transpose(2, 0, 1).astype('float32'))\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883c9ba-4e4c-4bd8-be2f-412a950d423b",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add831ba-9a5d-4a6b-813c-cfcbe917ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb5eb644-3d27-40f3-b75f-e8517faf35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    win_size = 32 * random.randint(7, 10)\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "\n",
    "        #albu.ShiftScaleRotate(scale_limit=0.0, rotate_limit=45, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        #albu.PadIfNeeded(min_height=win_size, min_width=win_size, always_apply=True, border_mode=4),\n",
    "        albu.RandomCrop(height=win_size, width=win_size, always_apply=True),\n",
    "\n",
    "        #albu.GaussNoise(p=0.2, var_limit=1.0),\n",
    "        #albu.Perspective(p=0.5),\n",
    "\n",
    "        #albu.OneOf(\n",
    "        #    [\n",
    "        #        #albu.CLAHE(p=1), # required int8 images\n",
    "        #        albu.RandomBrightnessContrast(p=1),\n",
    "        #        #albu.RandomGamma(p=1),\n",
    "        #        #albu.HueSaturationValue(p=1),\n",
    "        #    ],\n",
    "        #    p=0.9,\n",
    "        #),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=(3, 7), p=1),\n",
    "                albu.MotionBlur(blur_limit=(3, 7), p=1),\n",
    "            ],\n",
    "            p=0.25,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(384, 480)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe86ab2-173c-4ae2-84e3-161213c60ce7",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b03d0cbb-6436-4b1d-9ab7-ca856e3de540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils\n",
    "import torch.nn as nn\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3381ab39-295e-49e3-b70a-e1fdad7fe3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['burrow']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cuda'# 'cpu'# \n",
    "USE_PARALLEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "504c232a-505a-4d9a-9b04-c422e12db46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#list(itertools.combinations(avail_suffix, len(avail_suffix)))\n",
    "suffix_combinations = list()\n",
    "for n in range(1, len(avail_suffix) + 1):\n",
    "    suffix_combinations += list(itertools.combinations(avail_suffix, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cbaf71a-c873-4635-a5bf-ccc13762ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDIR = './cnn_results_unetplus/'\n",
    "if not os.path.exists(outDIR):\n",
    "    os.mkdir(outDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc1a920a-5b90-45e2-8d2a-26ef7ddce7a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "['rgb']\n",
      "\n",
      "Epoch: 1\n",
      "train:   0%|          | 0/56 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m61\u001b[39m):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[0;32m---> 91\u001b[0m     train_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     valid_logs \u001b[38;5;241m=\u001b[39m valid_epoch\u001b[38;5;241m.\u001b[39mrun(valid_loader)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# do something (save model, change lr, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/segmentation_models_pytorch/utils/train.py:51\u001b[0m, in \u001b[0;36mEpoch.run\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m     50\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 51\u001b[0m     loss, y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# update loss logs\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/segmentation_models_pytorch/utils/train.py:90\u001b[0m, in \u001b[0;36mTrainEpoch.batch_update\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 90\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(prediction, y)\n\u001b[1;32m     92\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:158\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule must have its parameters and buffers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon device \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (device_ids[0]) but found one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem on device: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj, t\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m--> 158\u001b[0m inputs, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:175\u001b[0m, in \u001b[0;36mDataParallel.scatter\u001b[0;34m(self, inputs, kwargs, device_ids)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, kwargs, device_ids):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:44\u001b[0m, in \u001b[0;36mscatter_kwargs\u001b[0;34m(inputs, kwargs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_kwargs\u001b[39m(inputs, kwargs, target_gpus, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Scatter with support for kwargs dictionary\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     45\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m scatter(kwargs, target_gpus, dim) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(kwargs):\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:36\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# After scatter_map is called, a scatter_map cell will exist. This cell\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# has a reference to the actual function scatter_map, which has references\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# to a closure that has a reference to the scatter_map cell (because the\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# fn is recursive). To avoid this reference cycle, we set the function to\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# None, clearing the cell\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     scatter_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:23\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:19\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_map\u001b[39m(obj):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mScatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_namedtuple(obj):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:96\u001b[0m, in \u001b[0;36mScatter.forward\u001b[0;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Perform CPU to GPU copies in a background stream\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     streams \u001b[38;5;241m=\u001b[39m [_get_stream(device) \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n\u001b[0;32m---> 96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Synchronize with the copy stream\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/project/cper_neon_aop/conda_envs/py_seg_env/lib/python3.10/site-packages/torch/nn/parallel/comm.py:189\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(tensor, devices, chunk_sizes, dim, streams, out)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "for suffix_sub in suffix_combinations:\n",
    "    suffix_list = list(suffix_sub)\n",
    "    print('\\n\\n----------------------------------------------------------')\n",
    "    print(suffix_list)\n",
    "    if os.path.exists(outDIR + 'best_model_' + '_'.join(suffix_list) + '_validation.txt'):\n",
    "        print('skipping - already trained.')\n",
    "        continue\n",
    "    else:\n",
    "        train_dataset = Dataset(\n",
    "            train_ids,\n",
    "            suffix_list,\n",
    "            DATA_FOLDER,\n",
    "            LABEL_FOLDER,\n",
    "            augmentation=get_training_augmentation(),\n",
    "            #preprocessing=get_preprocessing(preprocessing_fn),\n",
    "            classes=CLASSES)\n",
    "\n",
    "        valid_dataset = Dataset(\n",
    "            valid_ids,\n",
    "            suffix_list,\n",
    "            DATA_FOLDER,\n",
    "            LABEL_FOLDER,\n",
    "            #augmentation=get_validation_augmentation(),\n",
    "            #preprocessing=get_preprocessing(preprocessing_fn),\n",
    "            classes=CLASSES)\n",
    "\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=6, shuffle=False,\n",
    "                                  drop_last=True, num_workers=6, pin_memory=False)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=6, pin_memory=False)\n",
    "\n",
    "        # create segmentation model with pretrained encoder\n",
    "        model = smp.UnetPlusPlus(\n",
    "            encoder_name=ENCODER, \n",
    "            encoder_weights=ENCODER_WEIGHTS, \n",
    "            classes=len(CLASSES), \n",
    "            activation=ACTIVATION,\n",
    "            in_channels=train_dataset[0][0].shape[0],\n",
    "        )\n",
    "        \n",
    "        if USE_PARALLEL:\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "        # Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "        # IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "        loss = smp.losses.MCCLoss()\n",
    "        loss.__name__ = 'mccloss'\n",
    "        metrics = [\n",
    "            utils.metrics.IoU(threshold=0.5),\n",
    "            utils.metrics.Accuracy(threshold=0.5),\n",
    "            utils.metrics.Precision(threshold=0.5),\n",
    "            utils.metrics.Recall(threshold=0.5),\n",
    "            utils.metrics.Fscore(threshold=0.5)\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.Adam([ \n",
    "            dict(params=model.parameters(), lr=0.0001),\n",
    "        ])\n",
    "\n",
    "        # create epoch runners \n",
    "        # it is a simple loop of iterating over dataloader`s samples\n",
    "        train_epoch = utils.train.TrainEpoch(\n",
    "            model, \n",
    "            loss=loss, \n",
    "            metrics=metrics, \n",
    "            optimizer=optimizer,\n",
    "            device=DEVICE,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        valid_epoch = utils.train.ValidEpoch(\n",
    "            model, \n",
    "            loss=loss, \n",
    "            metrics=metrics, \n",
    "            device=DEVICE,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # train model for up to 60 epochs\n",
    "\n",
    "        max_score = 0\n",
    "        max_score_train = 0\n",
    "        no_improve = 0\n",
    "\n",
    "        for i in range(1, 61):\n",
    "\n",
    "            print('\\nEpoch: {}'.format(i))\n",
    "            train_logs = train_epoch.run(train_loader)\n",
    "            valid_logs = valid_epoch.run(valid_loader)\n",
    "\n",
    "            # do something (save model, change lr, etc.)\n",
    "            if max_score < valid_logs['fscore']:\n",
    "                max_score = valid_logs['fscore']\n",
    "                max_score_train = train_logs['fscore']\n",
    "                torch.save(model, outDIR + 'best_model_' + '_'.join(suffix_list) + '.pth')\n",
    "                valid_logs['best_epoch'] = i\n",
    "                best_valid_logs = valid_logs.copy()\n",
    "                print('Model saved!')\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                print('No improvement in ' + str(no_improve) + ' epochs. Model not saved.')\n",
    "\n",
    "            if i > 15:\n",
    "                if no_improve >= 5:\n",
    "                    if (train_logs['fscore'] - max_score_train) < 0.05:\n",
    "                        print('More than 5 epochs without validation improvement while learning rate <= 1e-5 and training improvement < 0.05...ending training')\n",
    "                        with open(outDIR + 'best_model_' + '_'.join(suffix_list) + '_validation.txt','w') as data: \n",
    "                            data.write(str(best_valid_logs))\n",
    "                        break\n",
    "                    elif (train_logs['fscore'] - max_score_train) < 0.10 and no_improve == 15:\n",
    "                        print('15 epochs without validation improvement while learning rate <= 1e-5...ending training')\n",
    "                        with open(outDIR + 'best_model_' + '_'.join(suffix_list) + '_validation.txt','w') as data: \n",
    "                            data.write(str(best_valid_logs))\n",
    "                        break\n",
    "            if i == 60:\n",
    "                with open(outDIR + 'best_model_' + '_'.join(suffix_list) + '_validation.txt','w') as data: \n",
    "                            data.write(str(best_valid_logs))\n",
    "                        \n",
    "            if i%15 == 0:\n",
    "                no_improve = 0\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.1\n",
    "                print('Decrease decoder learning rate by factor of 10')\n",
    "\n",
    "        del model, train_epoch, valid_epoch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7ad66-30bc-479d-88df-0b2a8c283871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All processing complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddad873-d854-45ff-b899-2ea1a0c0a360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_seg_env",
   "language": "python",
   "name": "py_seg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
