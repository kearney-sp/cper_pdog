{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102c40a0-b7f9-4dc6-ac35-9fc1cbc7b49a",
   "metadata": {},
   "source": [
    "#### Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805b4878-fed6-4dea-af0b-bbf21a903c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9be7cd-8018-4037-8f5e-ec5817286217",
   "metadata": {},
   "source": [
    "#### Set processing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70753fb-f839-4116-86c3-52b156c8b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "USE_PARALLEL = False\n",
    "res = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6450ba-f2a8-407b-b4b3-a48d87b00571",
   "metadata": {},
   "source": [
    "### Prep lists of input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0d91b2-cef1-4c19-b08c-bdc1f682d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from random import sample, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab307572-c831-44de-a43a-d7d872a397d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avail_suffix = ['rgb', 'dsm', 'ndvi', 'tpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40650c2d-2675-4431-a3d3-a7124ee8ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "#os.chdir('/project/cper_neon_aop/cper_pdog_uas')\n",
    "\n",
    "# set directories for training data and labels\n",
    "DATA_FOLDER = './cnn_train_images_' + str(res) + 'cm/{}_{}.tif'\n",
    "LABEL_FOLDER = './cnn_train_labels_' + str(res) + 'cm/{}_labels.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964c07d0-dd79-486d-9a45-26cf7b84d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csvs with training information\n",
    "df_tiles = pd.read_csv('train_tiles/train_bboxes_all_assigned.csv')\n",
    "df_polys = pd.read_csv('train_polys/train_polys_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2cc6e61-4166-46f0-80aa-70eca205075c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train\n",
       "1    76\n",
       "0    57\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tiles.value_counts('Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92bddfb-90ba-468b-a358-9ee021cb0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all ids to be used\n",
    "label_files = glob(LABEL_FOLDER.replace('{}', '*'))\n",
    "all_ids = [re.sub('_labels.tif', '', os.path.basename(f)) for f in label_files]\n",
    "all_tiles = list(set(['_'.join(y.split('_')[2:]) for y in all_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09eb2d92-e56e-431a-8b11-2d599336ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate training and test data and get paths to files\n",
    "all_files = glob(DATA_FOLDER.replace('{}', '*'))\n",
    "all_train_tiles = [x for x in df_tiles.apply(lambda x: '_'.join([x.Pasture, x.Tile]) if x.Train == 1 else '', axis=1) if x != '' and x in all_tiles]\n",
    "test_tiles = list(set(all_tiles) - set(all_train_tiles))\n",
    "\n",
    "all_train_ids = [x for x in all_ids if '_'.join(x.split('_')[-3:]) in all_train_tiles]\n",
    "test_ids = list(set(all_ids) - set(all_train_ids))\n",
    "\n",
    "seed(321)\n",
    "valid_ids = sample(all_train_ids, int(np.ceil(len(all_train_ids)*0.3)))\n",
    "train_ids = list(set(all_train_ids) - set(valid_ids))\n",
    "\n",
    "train_files = [f for f in all_files if '_'.join(os.path.basename(f).split('_')[:-1]) in train_ids]\n",
    "valid_files = [f for f in all_files if '_'.join(os.path.basename(f).split('_')[:-1]) in valid_ids]\n",
    "test_files = [f for f in all_files if '_'.join(os.path.basename(f).split('_')[:-1]) in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c06f11-1701-4cc8-81be-d7c26c7a0ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_ids = df_tiles[(df_tiles['trainer'] != 'Nick') &\n",
    "                    (df_tiles['Digitize'] == 1)].apply(lambda x: '_'.join([x.Pasture, x.Tile]), axis=1)\n",
    "#all_tiles#\n",
    "[x for x in all_tiles if x not in tile_ids.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ace3e5-8ab0-4e18-8ea6-b3d00c690c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in tile_ids.to_list() if x not in all_tiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180703c2-6582-4b02-8ef2-78e9de7db704",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "Writing helper class for data extraction, tranformation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100b2c6e-9f49-42d8-9346-4000ffc9c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "616adccf-9bcb-421d-9bb6-04d16c48815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        ids (list): list of unique ids for all images\n",
    "        images_path (str): path to data images\n",
    "        masks_path (str): path to label masks\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    CLASSES = ['other', 'burrow']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            ids,\n",
    "            suffix_list,\n",
    "            images_path,\n",
    "            masks_path, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            suffix_dict = {\n",
    "        'rgb': {'channels': 3,\n",
    "                'dtype': 'uint8'},\n",
    "        'tpi': {'channels': 1,\n",
    "                'dtype': 'float32'},\n",
    "        'dsm': {'channels': 1,\n",
    "                'dtype': 'float32'},\n",
    "        'shade': {'channels': 1,\n",
    "                  'dtype': 'float32'},\n",
    "        'ndvi': {'channels': 1,\n",
    "                  'dtype': 'float32'}\n",
    "    }\n",
    "    ):\n",
    "        # get IDs as attribute\n",
    "        self.ids = ids\n",
    "        \n",
    "        # get suffix info\n",
    "        self.suffix_dict = suffix_dict\n",
    "        \n",
    "        # get list of suffixes as attribute\n",
    "        self.suffix_list = suffix_list\n",
    "        \n",
    "        # List of files\n",
    "        self.images_fps = []\n",
    "        self.masks_fps = [masks_path.format(id) for id in ids]\n",
    "        for id in ids:\n",
    "            self.images_fps.append({s: images_path.format(id, s) for s in suffix_list})\n",
    "            \n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image_list = []\n",
    "        self.image_dict = {}\n",
    "        for s in self.suffix_list:\n",
    "            image_s = np.asarray(io.imread(self.images_fps[i][s]), dtype=self.suffix_dict[s]['dtype'])\n",
    "            if len(image_s.shape) == 2:\n",
    "                image_s = np.expand_dims(image_s, axis=-1)\n",
    "            image_s[np.isnan(image_s)] = 0\n",
    "            if s == 'ndvi':\n",
    "                image_s[image_s < 0] = 0\n",
    "            if self.preprocessing:\n",
    "                image_s = normalize_fn(image_s, s, image_stats)\n",
    "            image_list.append(image_s)\n",
    "            self.image_dict[s] = image_s\n",
    "        if len(image_list) == 1:\n",
    "            image = image_list[0]\n",
    "        else:\n",
    "            image = np.concatenate(image_list, axis=-1)\n",
    "        mask = np.asarray(io.imread(self.masks_fps[i]), dtype='float32')\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1)#.astype('float32')\n",
    "        #print('fetched: ', self.ids[i])\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # convert final image arrays to tensors\n",
    "        image = torch.from_numpy(image.transpose(2, 0, 1).astype('float32'))\n",
    "        mask = torch.from_numpy(mask.transpose(2, 0, 1).astype('float32'))\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883c9ba-4e4c-4bd8-be2f-412a950d423b",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "add831ba-9a5d-4a6b-813c-cfcbe917ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb5eb644-3d27-40f3-b75f-e8517faf35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    win_size = 32 * random.randint(7, 10)\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "\n",
    "        #albu.ShiftScaleRotate(scale_limit=0.0, rotate_limit=45, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        #albu.PadIfNeeded(min_height=win_size, min_width=win_size, always_apply=True, border_mode=4),\n",
    "        albu.RandomCrop(height=win_size, width=win_size, always_apply=True),\n",
    "\n",
    "        #albu.GaussNoise(p=0.2, var_limit=1.0),\n",
    "        #albu.Perspective(p=0.5),\n",
    "\n",
    "        #albu.OneOf(\n",
    "        #    [\n",
    "        #        #albu.CLAHE(p=1), # required int8 images\n",
    "        #        albu.RandomBrightnessContrast(p=1),\n",
    "        #        #albu.RandomGamma(p=1),\n",
    "        #        #albu.HueSaturationValue(p=1),\n",
    "        #    ],\n",
    "        #    p=0.9,\n",
    "        #),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=(3, 7), p=1),\n",
    "                albu.MotionBlur(blur_limit=(3, 7), p=1),\n",
    "            ],\n",
    "            p=0.25,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b60d62-7c39-45ef-ab6f-8f5e3f3ae4c8",
   "metadata": {},
   "source": [
    "### Create preprocessing function from training data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72be9696-8237-4642-87cc-995e975c33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e21ceb7f-8ac2-4df4-b48b-742f8bd8ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['burrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e7be348-39e1-4166-ae94-e3ac4fd8816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_stats = {\n",
    "    'rgb': {'min': 0.0,\n",
    "            'max': 255.0},\n",
    "    'ndvi': {'min': 0.0,\n",
    "             'max': 1.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0830a13-047e-4833-a179-16ec743f6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix_list in []:\n",
    "    train_dataset = Dataset(\n",
    "        train_ids,\n",
    "        suffix_list,\n",
    "        DATA_FOLDER,\n",
    "        LABEL_FOLDER,\n",
    "        classes=CLASSES)\n",
    "    #train_dataset[0][0].cpu().numpy().shape\n",
    "    min_list = []\n",
    "    max_list = []\n",
    "    for i in range(len(train_dataset)):\n",
    "        min_list.append(np.min(train_dataset[i][0].cpu().numpy()))\n",
    "        max_list.append(np.max(train_dataset[i][0].cpu().numpy()))\n",
    "    image_stats[suffix_list[0]] = {\n",
    "        'min': np.min(min_list),\n",
    "        'max': np.max(max_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13783e6f-e4e6-4326-9c89-3809e4b490f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_stats = pd.DataFrame(image_stats).reset_index().rename(\n",
    "    columns={'index': 'stat'})\n",
    "df_image_stats.to_csv('./_utils/image_stats_' + str(res) + 'cm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25dc94fb-f922-4f86-bc2e-e210ce592ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to normalize all data in range 0-1\n",
    "def normalize_fn(image, image_suffix, stats_dict):\n",
    "    if image_suffix in stats_dict.keys():\n",
    "        min_tmp = stats_dict[image_suffix]['min']\n",
    "        max_tmp = stats_dict[image_suffix]['max']\n",
    "    else:\n",
    "        # normalize to individual image if min/max stats not specified in dictionary\n",
    "        min_tmp = np.min(image)\n",
    "        max_tmp = np.max(image)\n",
    "    return (image - min_tmp) / (max_tmp - min_tmp)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6db58993-a91a-46c2-99b4-8d43a222d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_list = ['ndvi']\n",
    "train_dataset = Dataset(\n",
    "        train_ids,\n",
    "        suffix_list,\n",
    "        DATA_FOLDER,\n",
    "        LABEL_FOLDER,\n",
    "        #augmentation=get_training_augmentation(),\n",
    "        preprocessing=True,\n",
    "        classes=CLASSES)\n",
    "min_list_tmp = []\n",
    "for i in range(len(train_dataset)):\n",
    "    min_list_tmp.append(np.min(train_dataset[i][0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0704fc5-1876-47d0-8bf8-58069000c82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(min_list_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe86ab2-173c-4ae2-84e3-161213c60ce7",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b03d0cbb-6436-4b1d-9ab7-ca856e3de540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils\n",
    "import torch.nn as nn\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3381ab39-295e-49e3-b70a-e1fdad7fe3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cuda'# 'cpu'# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "504c232a-505a-4d9a-9b04-c422e12db46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#list(itertools.combinations(avail_suffix, len(avail_suffix)))\n",
    "suffix_combinations = list()\n",
    "for n in range(1, len(avail_suffix) + 1):\n",
    "    suffix_combinations += list(itertools.combinations(avail_suffix, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc1a920a-5b90-45e2-8d2a-26ef7ddce7a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Now running model: deeplabplus\n",
      "----------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "['rgb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth\" to /home/spkearney/.cache/torch/hub/checkpoints/inceptionv4-8e4777a0.pth\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:1346\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1346\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/http/client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/http/client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/http/client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m \n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/http/client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/http/client.py:1454\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m-> 1454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/ssl.py:501\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    498\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/ssl.py:1310\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1310\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# create segmentation model with pretrained encoder\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeeplabplus\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeepLabV3Plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENCODER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENCODER_WEIGHTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCLASSES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACTIVATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfpn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     50\u001b[0m     model \u001b[38;5;241m=\u001b[39m smp\u001b[38;5;241m.\u001b[39mFPN(\n\u001b[1;32m     51\u001b[0m         encoder_name\u001b[38;5;241m=\u001b[39mENCODER, \n\u001b[1;32m     52\u001b[0m         encoder_weights\u001b[38;5;241m=\u001b[39mENCODER_WEIGHTS, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m         in_channels\u001b[38;5;241m=\u001b[39mtrain_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     56\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/site-packages/segmentation_models_pytorch/decoders/deeplabv3/model.py:146\u001b[0m, in \u001b[0;36mDeepLabV3Plus.__init__\u001b[0;34m(self, encoder_name, encoder_depth, encoder_weights, encoder_output_stride, decoder_channels, decoder_atrous_rates, in_channels, classes, activation, upsampling, aux_params)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_output_stride \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m]:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoder output stride should be 8 or 16, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(encoder_output_stride))\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_output_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m DeepLabV3PlusDecoder(\n\u001b[1;32m    155\u001b[0m     encoder_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[1;32m    156\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39mdecoder_channels,\n\u001b[1;32m    157\u001b[0m     atrous_rates\u001b[38;5;241m=\u001b[39mdecoder_atrous_rates,\n\u001b[1;32m    158\u001b[0m     output_stride\u001b[38;5;241m=\u001b[39mencoder_output_stride,\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head \u001b[38;5;241m=\u001b[39m SegmentationHead(\n\u001b[1;32m    162\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[1;32m    163\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     upsampling\u001b[38;5;241m=\u001b[39mupsampling,\n\u001b[1;32m    167\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/site-packages/segmentation_models_pytorch/encoders/__init__.py:81\u001b[0m, in \u001b[0;36mget_encoder\u001b[0;34m(name, in_channels, depth, weights, output_stride, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong pretrained weights `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` for encoder `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`. Available options are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     76\u001b[0m                 weights,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m             )\n\u001b[1;32m     80\u001b[0m         )\n\u001b[0;32m---> 81\u001b[0m     encoder\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mmodel_zoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     83\u001b[0m encoder\u001b[38;5;241m.\u001b[39mset_in_channels(in_channels, pretrained\u001b[38;5;241m=\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m32\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/site-packages/torch/hub.py:727\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[1;32m    725\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[1;32m    726\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location)\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/site-packages/torch/hub.py:593\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    591\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    592\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.hub\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m--> 593\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m meta \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(meta, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetheaders\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[0;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    516\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 517\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    520\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    533\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 534\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    535\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py_seg_env/lib/python3.9/urllib/request.py:1349\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1347\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1350\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>"
     ]
    }
   ],
   "source": [
    "for mod in ['deeplabplus', 'fpn', 'manet', 'unetplus']:\n",
    "    print('\\n\\n----------------------------------------------------------')\n",
    "    print('----------------------------------------------------------')\n",
    "    print('Now running model: ' + mod)\n",
    "    print('----------------------------------------------------------')\n",
    "    outDIR = './cnn_results_' + mod + '_' + str(res) + 'cm/'\n",
    "    if not os.path.exists(outDIR):\n",
    "        os.mkdir(outDIR)\n",
    "    for suffix_sub in suffix_combinations:\n",
    "        suffix_list = list(suffix_sub)\n",
    "        print('\\n\\n----------------------------------------------------------')\n",
    "        print(suffix_list)\n",
    "        if os.path.exists(outDIR + 'best_model_' + '_'.join(suffix_list) + '_validation.txt'):\n",
    "            print('skipping - already trained.')\n",
    "            continue\n",
    "        else:\n",
    "            train_dataset = Dataset(\n",
    "                train_ids,\n",
    "                suffix_list,\n",
    "                DATA_FOLDER,\n",
    "                LABEL_FOLDER,\n",
    "                augmentation=get_training_augmentation(),\n",
    "                preprocessing=True,\n",
    "                classes=CLASSES)\n",
    "\n",
    "            valid_dataset = Dataset(\n",
    "                valid_ids,\n",
    "                suffix_list,\n",
    "                DATA_FOLDER,\n",
    "                LABEL_FOLDER,\n",
    "                #augmentation=get_validation_augmentation(),\n",
    "                preprocessing=True,\n",
    "                classes=CLASSES)\n",
    "\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=6, shuffle=False,\n",
    "                                      drop_last=True, num_workers=6, pin_memory=False)\n",
    "            valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=6, pin_memory=False)\n",
    "\n",
    "            # create segmentation model with pretrained encoder\n",
    "            if mod == 'deeplabplus':\n",
    "                model = smp.DeepLabV3Plus(\n",
    "                    encoder_name=ENCODER, \n",
    "                    encoder_weights=ENCODER_WEIGHTS, \n",
    "                    classes=len(CLASSES), \n",
    "                    activation=ACTIVATION,\n",
    "                    in_channels=train_dataset[0][0].shape[0],\n",
    "                )\n",
    "            elif mod == 'fpn':\n",
    "                model = smp.FPN(\n",
    "                    encoder_name=ENCODER, \n",
    "                    encoder_weights=ENCODER_WEIGHTS, \n",
    "                    classes=len(CLASSES), \n",
    "                    activation=ACTIVATION,\n",
    "                    in_channels=train_dataset[0][0].shape[0],\n",
    "                )\n",
    "            elif mod == 'manet':\n",
    "                model = smp.MAnet(\n",
    "                    encoder_name=ENCODER, \n",
    "                    encoder_weights=ENCODER_WEIGHTS, \n",
    "                    classes=len(CLASSES), \n",
    "                    activation=ACTIVATION,\n",
    "                    in_channels=train_dataset[0][0].shape[0],\n",
    "                )\n",
    "            elif mod == 'unetplus':\n",
    "                model = smp.UnetPlusPlus(\n",
    "                    encoder_name=ENCODER, \n",
    "                    encoder_weights=ENCODER_WEIGHTS, \n",
    "                    classes=len(CLASSES), \n",
    "                    activation=ACTIVATION,\n",
    "                    in_channels=train_dataset[0][0].shape[0],\n",
    "                )\n",
    "            else:\n",
    "                print('ERROR: model \"' + mod + '\" not found!')\n",
    "                break\n",
    "                \n",
    "            if USE_PARALLEL:\n",
    "                model = nn.DataParallel(model)\n",
    "\n",
    "            preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "            # Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "            # IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "            loss = smp.losses.MCCLoss()\n",
    "            loss.__name__ = 'mccloss'\n",
    "            metrics = [\n",
    "                utils.metrics.IoU(threshold=0.5),\n",
    "                utils.metrics.Accuracy(threshold=0.5),\n",
    "                utils.metrics.Precision(threshold=0.5),\n",
    "                utils.metrics.Recall(threshold=0.5),\n",
    "                utils.metrics.Fscore(threshold=0.5)\n",
    "            ]\n",
    "\n",
    "            optimizer = torch.optim.Adam([ \n",
    "                dict(params=model.parameters(), lr=0.0001),\n",
    "            ])\n",
    "\n",
    "            # create epoch runners \n",
    "            # it is a simple loop of iterating over dataloader`s samples\n",
    "            train_epoch = utils.train.TrainEpoch(\n",
    "                model, \n",
    "                loss=loss, \n",
    "                metrics=metrics, \n",
    "                optimizer=optimizer,\n",
    "                device=DEVICE,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "            valid_epoch = utils.train.ValidEpoch(\n",
    "                model, \n",
    "                loss=loss, \n",
    "                metrics=metrics, \n",
    "                device=DEVICE,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "            # train model for up to 60 epochs\n",
    "\n",
    "            max_score = 0\n",
    "            max_score_train = 0\n",
    "            no_improve = 0\n",
    "\n",
    "            for i in range(1, 61):\n",
    "\n",
    "                print('\\nEpoch: {}'.format(i))\n",
    "                train_logs = train_epoch.run(train_loader)\n",
    "                valid_logs = valid_epoch.run(valid_loader)\n",
    "                valid_logs['fscore_old'] = valid_logs['fscore']\n",
    "                beta = 1.0\n",
    "                valid_logs['fscore'] = ((1 + beta) * valid_logs['recall'] * valid_logs['precision']) / (valid_logs['recall'] + beta * valid_logs['precision'])                # do something (save model, change lr, etc.)\n",
    "                train_logs['fscore_old'] = train_logs['fscore']\n",
    "                beta = 1.0\n",
    "                train_logs['fscore'] = ((1 + beta) * train_logs['recall'] * train_logs['precision']) / (train_logs['recall'] + beta * train_logs['precision'])                # do something (save model, change lr, etc.)\n",
    "                \n",
    "                if max_score < valid_logs['fscore']:\n",
    "                    max_score = valid_logs['fscore']\n",
    "                    max_score_train = train_logs['fscore']\n",
    "                    torch.save(model, outDIR + 'best_model_' + '_'.join(suffix_list) + '.pth')\n",
    "                    valid_logs['best_epoch'] = i\n",
    "                    valid_logs['resolution'] = float(res)\n",
    "                    best_valid_logs = valid_logs.copy()\n",
    "                    print('Model saved!')\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    print('No improvement in ' + str(no_improve) + ' epochs. Model not saved.')\n",
    "\n",
    "                if i > 15:\n",
    "                    if no_improve >= 5:\n",
    "                        if (train_logs['fscore'] - max_score_train) < 0.05:\n",
    "                            print('More than 5 epochs without validation improvement while learning rate <= 1e-5 and training improvement < 0.05...ending training')\n",
    "                            with open(outDIR + 'best_model_' + '_'.join(suffix_list) + '_validation.txt','w') as data: \n",
    "                                data.write(str(best_valid_logs))\n",
    "                            break\n",
    "                        elif (train_logs['fscore'] - max_score_train) < 0.15 and no_improve == 15:\n",
    "                            print('15 epochs without validation improvement while learning rate <= 1e-5...ending training')\n",
    "                            with open(outDIR + 'best_model_' + '_'.join(suffix_list) + '_validation.txt','w') as data: \n",
    "                                data.write(str(best_valid_logs))\n",
    "                            break\n",
    "                if i == 60:\n",
    "                    with open(outDIR + 'best_model_' + '_'.join(suffix_list) + '_validation.txt','w') as data: \n",
    "                                data.write(str(best_valid_logs))\n",
    "\n",
    "                if i%15 == 0:\n",
    "                    no_improve = 0\n",
    "                    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.1\n",
    "                    print('Decrease decoder learning rate by factor of 10')\n",
    "\n",
    "            del model, train_epoch, valid_epoch\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7ad66-30bc-479d-88df-0b2a8c283871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All processing complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a462ad6-3efd-40a9-a767-899e5b16c1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
