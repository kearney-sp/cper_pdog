{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742871d-0259-4827-8dd5-9f1a7c98e868",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, Point\n",
    "import itertools\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio import mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.enums import Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import label, regionprops\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import shapely.wkt as wkt\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils\n",
    "from tqdm import tqdm\n",
    "from skimage.measure import label, regionprops\n",
    "import os\n",
    "import dask\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "from rioxarray.merge import merge_arrays\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "\n",
    "import distributed\n",
    "dask.config.set({\"distributed.nanny.environ.MALLOC_TRIM_THRESHOLD_\": 0})\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "import ctypes\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)\n",
    "\n",
    "os.environ[\"MALLOC_TRIM_THRESHOLD_\"] = str(dask.config.get(\"distributed.nanny.environ.MALLOC_TRIM_THRESHOLD_\"))\n",
    "\n",
    "print(os.environ[\"MALLOC_TRIM_THRESHOLD_\"])\n",
    "\n",
    "#os.environ[\"MALLOC_TRIM_THRESHOLD_\"] = '0'\n",
    "\n",
    "import xarray as xr\n",
    "import rioxarray as riox\n",
    "from xrspatial import convolution, focal, hillshade\n",
    "from skimage.transform import resize\n",
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "cluster = LocalCluster(n_workers=8, threads_per_worker=2, processes=True)\n",
    "client = Client(cluster)\n",
    "client.amm.start()\n",
    "\n",
    "# create function to normalize all data in range 0-1\n",
    "def normalize_fn(image, image_suffix, stats_dict):\n",
    "    if image_suffix in stats_dict.keys():\n",
    "        min_tmp = stats_dict[image_suffix]['min']\n",
    "        max_tmp = stats_dict[image_suffix]['max']\n",
    "    else:\n",
    "        # normalize to individual image if min/max stats not specified in dictionary\n",
    "        min_tmp = np.min(image)\n",
    "        max_tmp = np.max(image)\n",
    "    return (image - min_tmp) / (max_tmp - min_tmp)\n",
    "\n",
    "def calc_tpi(dtm, inner_r, outer_r, interpolate=True, values=True, bounds=(-2.0, 2.0)):\n",
    "    cellsize_x, cellsize_y = convolution.calc_cellsize(dtm)\n",
    "    kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_r, inner_r)\n",
    "    tpi = dtm - focal.apply(dtm, kernel)\n",
    "    tpi = tpi.rio.write_nodata(-9999.)\n",
    "    tpi = tpi.where((tpi > bounds[0]) & (tpi < bounds[1]))\n",
    "    if interpolate:\n",
    "        if tpi.isnull().any().values:\n",
    "            tpi = tpi.rio.interpolate_na(method='nearest')\n",
    "    if values:\n",
    "        return tpi.values\n",
    "    else:\n",
    "        return tpi\n",
    "\n",
    "def calc_ndvi(ms, interpolate=True, values=True):\n",
    "    ndvi = (ms.sel(band=4).astype('float32') - ms.sel(band=3).astype('float32'))\\\n",
    "            / (ms.sel(band=4).astype('float32') + ms.sel(band=3).astype('float32'))\n",
    "    ndvi = ndvi.rio.write_nodata(-9999.)\n",
    "    ndvi = ndvi.where(ndvi != -9999.)\n",
    "    if interpolate:\n",
    "        if ndvi.isnull().any().values:\n",
    "            ndvi = ndvi.rio.interpolate_na(method='nearest')\n",
    "    if values:\n",
    "        return ndvi.values\n",
    "    else:\n",
    "        return ndvi\n",
    "\n",
    "from collections import namedtuple\n",
    "from operator import mul\n",
    "\n",
    "try:\n",
    "    reduce = reduce\n",
    "except NameError:\n",
    "    from functools import reduce # py3k\n",
    "\n",
    "Info = namedtuple('Info', 'start height')\n",
    "\n",
    "def max_size(mat, value=0):\n",
    "    \"\"\"Find height, width of the largest rectangle containing all `value`'s.\n",
    "    For each row solve \"Largest Rectangle in a Histrogram\" problem [1]:\n",
    "    [1]: http://blog.csdn.net/arbuckle/archive/2006/05/06/710988.aspx\n",
    "    \"\"\"\n",
    "    it = iter(mat)\n",
    "    hist = [(el==value) for el in next(it, [])]\n",
    "    max_size = max_rectangle_size(hist)\n",
    "    for row in it:\n",
    "        hist = [(1+h) if el == value else 0 for h, el in zip(hist, row)]\n",
    "        max_size = max(max_size, max_rectangle_size(hist), key=area)\n",
    "    return max_size\n",
    "\n",
    "def max_rectangle_size(histogram):\n",
    "    \"\"\"Find height, width of the largest rectangle that fits entirely under\n",
    "    the histogram.\n",
    "    >>> f = max_rectangle_size\n",
    "    >>> f([5,3,1])\n",
    "    (3, 2)\n",
    "    >>> f([1,3,5])\n",
    "    (3, 2)\n",
    "    >>> f([3,1,5])\n",
    "    (5, 1)\n",
    "    >>> f([4,8,3,2,0])\n",
    "    (3, 3)\n",
    "    >>> f([4,8,3,1,1,0])\n",
    "    (3, 3)\n",
    "    >>> f([1,2,1])\n",
    "    (1, 3)\n",
    "    Algorithm is \"Linear search using a stack of incomplete subproblems\" [1].\n",
    "    [1]: http://blog.csdn.net/arbuckle/archive/2006/05/06/710988.aspx\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    top = lambda: stack[-1]\n",
    "    max_size = (0, 0) # height, width of the largest rectangle\n",
    "    pos = 0 # current position in the histogram\n",
    "    for pos, height in enumerate(histogram):\n",
    "        start = pos # position where rectangle starts\n",
    "        while True:\n",
    "            if not stack or height > top().height:\n",
    "                stack.append(Info(start, height)) # push\n",
    "            elif stack and height < top().height:\n",
    "                max_size = max(max_size, (top().height, (pos - top().start)),\n",
    "                               key=area)\n",
    "                start, _ = stack.pop()\n",
    "                continue\n",
    "            break # height == top().height goes here\n",
    "\n",
    "    pos += 1\n",
    "    for start, height in stack:\n",
    "        max_size = max(max_size, (height, (pos - start)), key=area)\n",
    "\n",
    "    return max_size\n",
    "\n",
    "def area(size):\n",
    "    return reduce(mul, size)\n",
    "\n",
    "outDIR = './cnn_pred_results/'\n",
    "if not os.path.exists(outDIR):\n",
    "    os.mkdir(outDIR)\n",
    "\n",
    "ENCODER = 'resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['burrow']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda' #'cuda'# 'cpu'# \n",
    "model_fnl = 'deeplabplus'\n",
    "res_fnl = 5\n",
    "inputs_fnl = ['rgb', 'tpi', 'ndvi'] \n",
    "preprocess = True\n",
    "prob_thresh = 0.5\n",
    "\n",
    "size_dict = {\n",
    "    2: {'tile_size': 256,\n",
    "        'buff_size': 64},\n",
    "    5: {'tile_size': 192,\n",
    "        'buff_size': 48},\n",
    "    10: {'tile_size': 128,\n",
    "         'buff_size': 32},\n",
    "    15: {'tile_size': 96,\n",
    "         'buff_size': 16},\n",
    "    30: {'tile_size': 64,\n",
    "         'buff_size': 16}\n",
    "}\n",
    "\n",
    "#past_subset = None\n",
    "past_subset = ['22E', 'CN']\n",
    "\n",
    "img_f_dict = {\n",
    "    '5W': {\n",
    "        'rgb': ['/mnt/d/202109/outputs/202109_5W_RGB/CPER_202109_5W_RGB_ortho.tif'],\n",
    "        'ms': ['/mnt/d/202109/outputs/202109_5W_MS/CPER_202109_5W_MS_ortho.tif'],\n",
    "        'dsm': ['/mnt/d/202109/outputs/202109_5W_RGB/CPER_202109_5W_RGB_dsm.tif']\n",
    "    },\n",
    "    '29-30': {\n",
    "        'rgb': ['/mnt/d/202109/outputs/202109_29_30_RGB/CPER_202109_29_30_RGB_ortho.tif',\n",
    "                '/mnt/d/202109/outputs/202109_29_30_RGB/CPER_202109_29_30_RGB_ortho.tif'],\n",
    "        'ms': ['/mnt/d/202109/outputs/202109_29_30_MS/CPER_202109_29_30_North_MS_ortho.tif',\n",
    "              '/mnt/d/202109/outputs/202109_29_30_MS/CPER_202109_29_30_South_MS_ortho.tif'],\n",
    "        'dsm': ['/mnt/d/202109/outputs/202109_29_30_RGB/CPER_202109_29_30_RGB_DSM.tif',\n",
    "               '/mnt/d/202109/outputs/202109_29_30_RGB/CPER_202109_29_30_RGB_DSM.tif']\n",
    "    },\n",
    "    '22W': {\n",
    "        'rgb': ['/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight1_RGB_ortho.tif',\n",
    "               '/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight2_RGB_ortho.tif'],\n",
    "        'ms': ['/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight1_MS_ortho.tif',\n",
    "               '/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight2_MS_ortho.tif'],\n",
    "        'dsm': ['/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight1_RGB_DSM.tif',\n",
    "                '/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight2_RGB_DSM.tif']\n",
    "    },\n",
    "    '22E': {\n",
    "        'rgb': ['/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight1_RGB_ortho.tif',\n",
    "               '/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight2_RGB_ortho.tif',\n",
    "               '/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight3_RGB_ortho.tif'],\n",
    "        'ms': ['/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight1_MS_ortho.tif',\n",
    "               '/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight2_MS_ortho.tif',\n",
    "              '/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight2_MS_ortho.tif'],\n",
    "        'dsm': ['/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight1_RGB_DSM.tif',\n",
    "               '/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight2_RGB_DSM.tif',\n",
    "               '/mnt/d/202109/outputs/202109_22EW/CPER_202109_22EW_Flight3_RGB_DSM.tif']\n",
    "    },\n",
    "    'CN': {\n",
    "        'rgb': ['/mnt/d/202109/outputs/202109_CN_RGB/Orthos/CPER_CN_Flight2_202109_RGB_ortho.tif',\n",
    "               '/mnt/d/202109/outputs/202109_CN_RGB/Orthos/CPER_CN_Flight3_202109_RGB_ortho.tif',\n",
    "               '/mnt/d/202109/outputs/202109_CN_RGB/Orthos/CPER_CN_Flight4_202109_RGB_ortho.tif',\n",
    "               '/mnt/d/202109/outputs/202109_CN_RGB/Orthos/CPER_CN_Flight5_202109_RGB_ortho.tif',\n",
    "               '/mnt/d/202109/outputs/202109_CN_RGB/Orthos/CPER_CN_Flight5_202109_RGB_ortho.tif'],\n",
    "        'ms': ['/mnt/d/202109/outputs/202109_CN_MS/CPER_202109_CN_Flight2_MS_ortho.tif',\n",
    "              '/mnt/d/202109/outputs/202109_CN_MS/CPER_202109_CN_Flight2_MS_ortho.tif',\n",
    "              '/mnt/d/202109/outputs/202109_CN_MS/CPER_202109_CN_Flight3_MS_ortho.tif',\n",
    "              '/mnt/d/202109/outputs/202109_CN_MS/CPER_202109_CN_Flight3_MS_ortho.tif',\n",
    "              '/mnt/d/202109/outputs/202109_CN_MS/CPER_202109_CN_Flight4_MS_ortho.tif',],\n",
    "        'dsm': ['/mnt/d/202109/outputs/202109_CN_RGB/DSMs/CPER_CN_Flight2_202109_RGB_DSM.tif',\n",
    "               '/mnt/d/202109/outputs/202109_CN_RGB/DSMs/CPER_CN_Flight3_202109_RGB_DSM.tif',\n",
    "               '/mnt/d/202109/outputs/202109_CN_RGB/DSMs/CPER_CN_Flight4_202109_RGB_DSM.tif',\n",
    "               '/mnt/d/202109/outputs/202109_CN_RGB/DSMs/CPER_CN_Flight5_202109_RGB_DSM.tif',\n",
    "                '/mnt/d/202109/outputs/202109_CN_RGB/DSMs/CPER_CN_Flight5_202109_RGB_DSM.tif']\n",
    "    }\n",
    "}\n",
    "\n",
    "if past_subset is not None:\n",
    "    img_f_dict_tmp = img_f_dict.copy()\n",
    "    img_f_dict = {}\n",
    "    for k in img_f_dict_tmp:\n",
    "         if k in past_subset:\n",
    "                img_f_dict[k] = img_f_dict_tmp[k]\n",
    "\n",
    "cper_f = '/mnt/c/Users/TBGPEA-Sean/Desktop/Pdogs_UAS/cper_pdog_pastures_2017_clip.shp'\n",
    "\n",
    "full_buff_size = 10\n",
    "full_tile_size = 150\n",
    "tile_size = size_dict[res_fnl]['tile_size']\n",
    "buff_size = size_dict[res_fnl]['buff_size']\n",
    "chunk_size = 250\n",
    "buff_size_m = np.ceil(buff_size * res_fnl * 0.01)\n",
    "\n",
    "# load best saved checkpoint\n",
    "if res_fnl == 2:\n",
    "    best_model = torch.load('./cnn_results_' + model_fnl + '_' + str(res_fnl) + 'cm/best_model_' + '_'.join(inputs_fnl) + '.pth')\n",
    "else:\n",
    "    best_model = torch.load('./cnn_results_' + model_fnl + '_' + str(res_fnl) + 'cm/best_model_' + '_'.join(inputs_fnl) + '_' + str(res_fnl) + 'cm.pth')\n",
    "\n",
    "if DEVICE == 'cpu':\n",
    "    best_model = best_model.cpu()\n",
    "best_model.eval()\n",
    "\n",
    "# load the image stats from the training data\n",
    "df_image_stats = pd.read_csv('./_utils/image_stats_2cm.csv').set_index('stat')\n",
    "\n",
    "# convert image stats dictionary to dataframe\n",
    "image_stats = {i: {'min': df_image_stats.loc['min', i],\n",
    "                   'max': df_image_stats.loc['max', i]} for i in df_image_stats.columns}\n",
    "\n",
    "cper_gdf = gpd.read_file(cper_f)\n",
    "\n",
    "for pasture in tqdm(img_f_dict):\n",
    "    print('\\n\\n----------\\nPasture: ' + pasture)\n",
    "\n",
    "    # get the bounding box of the pasture\n",
    "    past_bbox = cper_gdf[cper_gdf['Past_Name_'] == pasture].buffer(\n",
    "        full_buff_size).bounds.apply(lambda x: int(x))\n",
    "\n",
    "    total_bounds = {'xmin': past_bbox['minx'],\n",
    "                    'xmax': past_bbox['maxx'],\n",
    "                    'ymin': past_bbox['miny'],\n",
    "                    'ymax': past_bbox['maxy']}\n",
    "\n",
    "    n_row_tiles = int(np.ceil((total_bounds['ymax'] - total_bounds['ymin'])/full_tile_size))\n",
    "    n_col_tiles = int(np.ceil((total_bounds['xmax'] - total_bounds['xmin'])/full_tile_size))\n",
    "\n",
    "    outSHP = os.path.join(outDIR, 'burrow_pts_pred_' + '_'.join([pasture] + inputs_fnl + [str(res_fnl)]) + 'cm.shp')\n",
    "    if os.path.exists(outSHP):\n",
    "        gdf_out = gpd.read_file(outSHP)\n",
    "        r_ct_pred = len(gdf_out)\n",
    "        rc_completed = gdf_out.apply(lambda x: '_'.join([str(x.tile_row), str(x.tile_col)]), axis=1).unique()\n",
    "    elif os.path.exists(re.sub('.shp', '.csv', outSHP)):\n",
    "        gdf_out = pd.read_csv(re.sub('.shp', '.csv', outSHP))\n",
    "        r_ct_pred = len(gdf_out)\n",
    "        rc_completed = gdf_out.apply(lambda x: '_'.join([str(x.tile_row), str(x.tile_col)]), axis=1).unique()\n",
    "    else:\n",
    "        r_ct_pred = 0\n",
    "        gdf_out = gpd.GeoDataFrame()\n",
    "        rc_completed = []\n",
    "    tile_ct = 0\n",
    "    for full_r in range(n_row_tiles):\n",
    "        print('running row: ' + str(full_r + 1) + ' of ' + str(n_row_tiles))\n",
    "        for full_c in tqdm(range(n_col_tiles)):\n",
    "            if len(client.cluster.workers) < 8:\n",
    "                client.shutdown()\n",
    "                client.close()\n",
    "                cluster = LocalCluster(n_workers=8, threads_per_worker=2, processes=True)\n",
    "                client = Client(cluster)\n",
    "                client.amm.start()\n",
    "            if '_'.join([str(full_r), str(full_c)]) in rc_completed:\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    t0=time.time()\n",
    "\n",
    "                    ll_tile = [full_c * full_tile_size + total_bounds['xmin'],\n",
    "                               full_r * full_tile_size + total_bounds['ymin']]\n",
    "                    ul_tile = [ll_tile[0], ll_tile[1] + full_tile_size]\n",
    "                    ur_tile = [x + full_tile_size for x in ll_tile]\n",
    "                    lr_tile = [ll_tile[0] + full_tile_size, ll_tile[1]]\n",
    "                    tile_poly = Polygon([ll_tile, ul_tile, ur_tile, lr_tile])\n",
    "\n",
    "                    ll = [full_c * full_tile_size + total_bounds['xmin'] - buff_size_m,\n",
    "                          full_r * full_tile_size + total_bounds['ymin'] - buff_size_m]\n",
    "                    ul = [ll[0], ll[1] + full_tile_size + (buff_size_m * 2.0)]\n",
    "                    ur = [x + full_tile_size + (buff_size_m * 2.0) for x in ll]\n",
    "                    lr = [ll[0] + full_tile_size + (buff_size_m * 2.0), ll[1]]\n",
    "\n",
    "                    image_dict = {}\n",
    "                    newsize_r = int(round((ul[1] - ll[1]) / (res_fnl * 0.01), 0))\n",
    "                    newsize_c = int(round((lr[0] - ll[0]) / (res_fnl * 0.01), 0))\n",
    "                    if 'rgb' in inputs_fnl:\n",
    "                        t1=time.time()\n",
    "\n",
    "                        rgb_xr_list = []\n",
    "                        for rgb_f in img_f_dict[pasture]['rgb']:\n",
    "                            with riox.open_rasterio(rgb_f, masked=True) as rgb_src:\n",
    "                                rgb_xr_list.append(rgb_src.sel(band=slice(0, 3),\n",
    "                                                               x=slice(ll[0], lr[0]),\n",
    "                                                               y=slice(ul[1], ll[1]), \n",
    "                                                               drop=True))\n",
    "                        rgb_xr_list = [x.where(x != 255) for x in rgb_xr_list if not any([s == 0 for s in x.shape])]\n",
    "                        rgb_xr_list = [x.rio.write_nodata(-9999.).chunk({'band': -1,\n",
    "                                                                       'x': chunk_size,\n",
    "                                                                       'y': chunk_size}) for x in rgb_xr_list]\n",
    "\n",
    "                        rgb_xr = merge_arrays(rgb_xr_list,\n",
    "                                              bounds=(ll[0], ll[1], lr[0], ur[1]), \n",
    "                                              res=res_fnl*0.01, \n",
    "                                              crs=rio.CRS.from_epsg(32613),\n",
    "                                              method='max',\n",
    "                                              nodata=-9999.)\n",
    "                        rgb_xr = rgb_xr.where(rgb_xr != -9999., drop=True).chunk({'band': -1,\n",
    "                                                                       'x': chunk_size,\n",
    "                                                                       'y': chunk_size})\n",
    "                        if rgb_xr.isnull().any().values:\n",
    "                            rgb_xr = rgb_xr.rio.interpolate_na(method='nearest')\n",
    "\n",
    "                        image_dict['rgb'] = rgb_xr.values\n",
    "                        del rgb_xr, rgb_xr_list\n",
    "                    if 'dsm' in inputs_fnl or 'tpi' in inputs_fnl:\n",
    "\n",
    "                        dsm_xr_list = []\n",
    "                        for dsm_f in img_f_dict[pasture]['dsm']:\n",
    "                            with riox.open_rasterio(dsm_f, masked=True) as dsm_src:\n",
    "                                dsm_xr_list.append(dsm_src.sel(x=slice(ll[0], lr[0]),\n",
    "                                                               y=slice(ul[1], ll[1]), \n",
    "                                                               drop=True))\n",
    "\n",
    "                        dsm_xr_list = [x.where(x > 0, drop=True) for x in dsm_xr_list]\n",
    "                        dsm_xr_list = [x.squeeze() for x in dsm_xr_list if not any([s == 0 for s in x.shape])]\n",
    "                        dsm_xr_list = [x.rio.write_nodata(-9999.).chunk({'x': chunk_size,\n",
    "                                                                         'y': chunk_size}) for x in dsm_xr_list]\n",
    "\n",
    "                        dsm_xr = merge_arrays(dsm_xr_list,\n",
    "                                              bounds=(ll[0], ll[1], lr[0], ur[1]), \n",
    "                                              res=res_fnl*0.01, \n",
    "                                              crs=rio.CRS.from_epsg(32613),\n",
    "                                              method='max',\n",
    "                                              nodata=-9999.)\n",
    "                        dsm_xr = dsm_xr.where(dsm_xr > 0)\n",
    "                        dsm_xr = dsm_xr.where(dsm_xr != -9999.)\n",
    "                        \n",
    "                        if dsm_xr.isnull().any().values:\n",
    "                            dsm_xr = dsm_xr.rio.interpolate_na(method='nearest')\n",
    "                        \n",
    "                        if 'dsm' in inputs_fnl:\n",
    "                            image_dict['dsm'] = dsm_xr.values\n",
    "                    if 'tpi' in inputs_fnl: \n",
    "                        # prepare an annulus kernel with a ring at a distance from 5-10 cells away from focal point\n",
    "                        outer_radius = \"0.75m\"\n",
    "                        inner_radius = \"0.25m\"\n",
    "                        image_dict['tpi'] = calc_tpi(dsm_xr.chunk({'x': chunk_size,\n",
    "                                                                   'y': chunk_size}), \n",
    "                                                     inner_r=inner_radius, \n",
    "                                                     outer_r=outer_radius, \n",
    "                                                     interpolate=True,\n",
    "                                                     values=True)\n",
    "                        del dsm_xr, dsm_xr_list\n",
    "                    if 'ndvi' in inputs_fnl:\n",
    "                        t1 = time.time()\n",
    "                        ms_xr_list = []\n",
    "                        for ms_f in img_f_dict[pasture]['ms']:\n",
    "                            with riox.open_rasterio(ms_f, masked=True) as ms_src:\n",
    "                                ms_xr_list.append(ms_src.sel(band=[4, 3],\n",
    "                                                               x=slice(ll[0], lr[0]),\n",
    "                                                               y=slice(ul[1], ll[1]), \n",
    "                                                               drop=True))\n",
    "\n",
    "                        ms_xr_list = [x.where(x != 65535, drop=True) for x in ms_xr_list if not any([s == 0 for s in x.shape])]\n",
    "                        ms_xr_list = [x.rio.write_nodata(-9999.).chunk({'x': chunk_size,\n",
    "                                                                        'y': chunk_size}) for x in ms_xr_list]\n",
    "\n",
    "                        ms_xr = merge_arrays(ms_xr_list,\n",
    "                                              bounds=(ll[0], ll[1], lr[0], ur[1]), \n",
    "                                              res=res_fnl*0.01, \n",
    "                                              crs=rio.CRS.from_epsg(32613),\n",
    "                                              method='max',\n",
    "                                              nodata=-9999.)\n",
    "                        ms_xr = ms_xr.where(ms_xr != -9999.)\n",
    "\n",
    "                        image_dict['ndvi'] = calc_ndvi(ms_xr, values=True)\n",
    "                        del ms_xr, ms_xr_list\n",
    "\n",
    "                    if 'rgb' in image_dict:\n",
    "                        tshape = image_dict['rgb'].shape[1:]\n",
    "                    else:\n",
    "                        tshape = image_dict[inputs_fnl[0]].shape\n",
    "\n",
    "                    n_row_chunks = int(np.ceil(tshape[0]/tile_size))\n",
    "                    n_col_chunks = int(np.ceil(tshape[1]/tile_size))\n",
    "\n",
    "                    pr_mask = np.empty(tshape)\n",
    "                    for r in range(n_row_chunks):\n",
    "                        if (r + 1) * tile_size > tshape[0]:\n",
    "                            r_min = tshape[0] - tile_size\n",
    "                            r_max = tshape[0]\n",
    "                            r_max_comp = tshape[0]\n",
    "                        elif (r + 1) * tile_size + buff_size > tshape[0]:\n",
    "                            r_min = r * tile_size\n",
    "                            r_max = (r + 1) * tile_size\n",
    "                            r_max_comp = r_max\n",
    "                        else:\n",
    "                            r_min = r * tile_size\n",
    "                            r_max = (r + 1) * tile_size\n",
    "                            r_max_comp = r_max + buff_size\n",
    "                        for c in range(n_col_chunks):\n",
    "                            image_sub_dict = {}\n",
    "                            if (c + 1) * tile_size > tshape[1]:\n",
    "                                c_min = tshape[1] - tile_size\n",
    "                                c_max = tshape[1]\n",
    "                                c_max_comp = tshape[1]\n",
    "                            elif (c + 1) * tile_size + buff_size > tshape[1]:\n",
    "                                c_min = c * tile_size\n",
    "                                c_max = (c + 1) * tile_size\n",
    "                                c_max_comp = c_max\n",
    "                            else:\n",
    "                                c_min = c * tile_size\n",
    "                                c_max = (c + 1) * tile_size\n",
    "                                c_max_comp = c_max + buff_size\n",
    "                            for k in image_dict:\n",
    "                                if k == 'rgb':\n",
    "                                    image_sub_dict[k] = image_dict[k][:,\n",
    "                                                                      slice(max(0, r_min-buff_size), r_max_comp),\n",
    "                                                                      slice(max(0, c_min-buff_size), c_max_comp)].astype('float32')\n",
    "                                else:\n",
    "                                    image_sub_dict[k] = image_dict[k][slice(max(0, r_min-buff_size), r_max_comp),\n",
    "                                                                      slice(max(0, c_min-buff_size), c_max_comp)].astype('float32')\n",
    "                                if len(image_sub_dict[k].shape) == 2:\n",
    "                                    image_sub_dict[k] = np.expand_dims(image_sub_dict[k], 0)\n",
    "                                if np.all(np.isnan(image_sub_dict[k])):\n",
    "                                    continue\n",
    "                                elif np.any(np.isnan(image_sub_dict[k])):\n",
    "                                    for i in range(image_sub_dict[k].shape[0]):\n",
    "                                        if np.any(np.isnan(image_sub_dict[k][i, :, :])):\n",
    "                                            data = image_sub_dict[k][i, :, :].copy()\n",
    "                                            mask = np.where(~np.isnan(data))\n",
    "                                            interp = NearestNDInterpolator(np.transpose(mask), data[mask])\n",
    "                                            image_sub_dict[k][i, :, :] = interp(*np.indices(data.shape))\n",
    "                                            del data, mask, interp\n",
    "    \n",
    "                            if np.any([np.all(np.isnan(image_sub_dict[k])) for k in image_sub_dict]):\n",
    "                                pr_mask[r_min:r_max, c_min:c_max] = np.nan\n",
    "                                continue\n",
    "                            else:\n",
    "                                if preprocess:\n",
    "                                    for i in image_sub_dict:\n",
    "                                        image_sub_dict[i] = normalize_fn(image_sub_dict[i], i, image_stats)\n",
    "                                image_list = [image_sub_dict[i] for i in inputs_fnl]\n",
    "                                image_out = np.concatenate(image_list, axis=0)\n",
    "                                x_tensor = torch.from_numpy(image_out).to(DEVICE).unsqueeze(0)\n",
    "                                if type(best_model) == nn.DataParallel:\n",
    "                                    pred_tmp = best_model.module.predict(x_tensor).cpu().detach().numpy().squeeze() >= prob_thresh\n",
    "                                    buff_r_min = buff_size * int(r_min-buff_size > 0)\n",
    "                                    buff_r_max = buff_size * int(r_max+buff_size <= tshape[0])\n",
    "                                    buff_c_min = buff_size * int(c_min-buff_size > 0)\n",
    "                                    buff_c_max = buff_size * int(c_max+buff_size <= tshape[1])\n",
    "                                    if pred_tmp.shape[1] > tile_size:\n",
    "                                        pr_mask[r_min:r_max, c_min:c_max] = pred_tmp[buff_r_min:pred_tmp.shape[0]-buff_r_max,\n",
    "                                                                                     buff_c_min:pred_tmp.shape[1]-buff_c_max]\n",
    "                                    else:\n",
    "                                        pr_mask[r_min:r_max, c_min:c_max] = pred_tmp\n",
    "                                else:\n",
    "                                    pred_tmp = best_model.predict(x_tensor).cpu().detach().numpy().squeeze() >= prob_thresh\n",
    "                                    buff_r_min = buff_size * int(r_min-buff_size > 0)\n",
    "                                    buff_r_max = buff_size * int(r_max+buff_size <= tshape[0])\n",
    "                                    buff_c_min = buff_size * int(c_min-buff_size > 0)\n",
    "                                    buff_c_max = buff_size * int(c_max+buff_size <= tshape[1])\n",
    "                                    if pred_tmp.shape[1] > tile_size:\n",
    "                                        pr_mask[r_min:r_max, c_min:c_max] = pred_tmp[buff_r_min:pred_tmp.shape[0]-buff_r_max,\n",
    "                                                                                     buff_c_min:pred_tmp.shape[1]-buff_c_max]\n",
    "                                    else:\n",
    "                                        pr_mask[r_min:r_max, c_min:c_max] = pred_tmp\n",
    "                    if np.all(pr_mask == 0):\n",
    "                        gdf_tmp = gpd.GeoDataFrame(data=pd.DataFrame({'area': ''}, index=[r_ct_pred]))\n",
    "                        gdf_tmp['tile_row'] = full_r\n",
    "                        gdf_tmp['tile_col'] =  full_c\n",
    "                        gdf_tmp['tile_size'] = full_tile_size\n",
    "                        gdf_out = pd.concat([gdf_out, gdf_tmp])\n",
    "                        del gdf_tmp\n",
    "                        r_ct_pred += 1\n",
    "                    else:\n",
    "                        pr_labels = label(pr_mask)\n",
    "                        pr_regions = regionprops(pr_labels)\n",
    "                        pr_regions = [r for r in pr_regions if (r.area*(res_fnl/100)**2 > 0.05) & (r.area*(res_fnl/100)**2 < 5.0)]\n",
    "                        if len(pr_regions) == 0:\n",
    "                            gdf_tmp = gpd.GeoDataFrame(data=pd.DataFrame({'area': ''}, index=[r_ct_pred]))\n",
    "                            gdf_tmp['tile_row'] = full_r\n",
    "                            gdf_tmp['tile_col'] =  full_c\n",
    "                            gdf_tmp['tile_size'] = full_tile_size\n",
    "                            gdf_out = pd.concat([gdf_out, gdf_tmp])\n",
    "                            del gdf_tmp\n",
    "                            r_ct_pred += 1\n",
    "                        else:\n",
    "                            r_ct_tile = 0\n",
    "                            for r in pr_regions:\n",
    "                                gdf_tmp = gpd.GeoDataFrame(data=pd.DataFrame({'area': r.area}, \n",
    "                                                                             index=[r_ct_pred]), \n",
    "                                                           geometry=[Point([ll[0] + r.centroid[1]*(res_fnl*0.01),\n",
    "                                                                            ul[1] - r.centroid[0]*(res_fnl*0.01)])], \n",
    "                                                           crs='EPSG:32613')\n",
    "                                if gdf_tmp.geometry.within(tile_poly).values[0]:\n",
    "                                    gdf_tmp['tile_row'] = full_r\n",
    "                                    gdf_tmp['tile_col'] =  full_c\n",
    "                                    gdf_tmp['tile_size'] = full_tile_size\n",
    "                                    gdf_out = pd.concat([gdf_out, gdf_tmp])\n",
    "                                    r_ct_tile += 1\n",
    "                                if type(gdf_out) is pd.core.frame.DataFrame:\n",
    "                                    gdf_out = gpd.GeoDataFrame(gdf_out, geometry = gdf_out['geometry'])\n",
    "                                del gdf_tmp\n",
    "                                r_ct_pred += 1\n",
    "                            if r_ct_tile == 0:\n",
    "                                gdf_tmp = gpd.GeoDataFrame(data=pd.DataFrame({'area': ''}, index=[r_ct_pred]))\n",
    "                                gdf_tmp['tile_row'] = full_r\n",
    "                                gdf_tmp['tile_col'] =  full_c\n",
    "                                gdf_tmp['tile_size'] = full_tile_size\n",
    "                                gdf_out = pd.concat([gdf_out, gdf_tmp])\n",
    "                                del gdf_tmp\n",
    "                                r_ct_pred += 1\n",
    "                    if type(gdf_out) is pd.core.frame.DataFrame:\n",
    "                        gdf_out.to_csv(re.sub('.shp', '.csv', outSHP), index=False)\n",
    "                    else:\n",
    "                        gdf_out.to_file(outSHP)\n",
    "                    try:\n",
    "                        del pr_mask, pred_tmp, pr_labels, pr_regions, image_dict, image_sub_dict, image_list, image_out\n",
    "                    except NameError:\n",
    "                        pass\n",
    "                    gc.collect()\n",
    "                    client.run(gc.collect)\n",
    "                    client.run(trim_memory)\n",
    "                    if (tile_ct > 0) & (tile_ct % 15 == 0):\n",
    "                        try:\n",
    "                            client.restart(timeout=9)\n",
    "                            time.sleep(10)\n",
    "                        except TimeoutError:\n",
    "                            client.shutdown()\n",
    "                            client.close()\n",
    "                            cluster = LocalCluster(n_workers=8, threads_per_worker=2, processes=True)\n",
    "                            client = Client(cluster)\n",
    "                            client.amm.start()\n",
    "\n",
    "                    #client.restart()\n",
    "                except NoDataInBounds:\n",
    "                    gdf_tmp = gpd.GeoDataFrame(data=pd.DataFrame({'area': ''}, index=[r_ct_pred]))\n",
    "                    gdf_tmp['tile_row'] = full_r\n",
    "                    gdf_tmp['tile_col'] =  full_c\n",
    "                    gdf_tmp['tile_size'] = full_tile_size\n",
    "                    gdf_out = pd.concat([gdf_out, gdf_tmp])\n",
    "                    del gdf_tmp\n",
    "                    r_ct_pred += 1\n",
    "                    if type(gdf_out) is pd.core.frame.DataFrame:\n",
    "                        gdf_out.to_csv(re.sub('.shp', '.csv', outSHP), index=False)\n",
    "                    else:\n",
    "                        gdf_out.to_file(outSHP)\n",
    "                    continue\n",
    "            tile_ct += 1\n",
    "    print('Pasture-group finished!')\n",
    "    try:\n",
    "        client.restart(timeout=9)\n",
    "        time.sleep(10)\n",
    "    except TimeoutError:\n",
    "        client.shutdown()\n",
    "        client.close()\n",
    "        cluster = LocalCluster(n_workers=8, threads_per_worker=2, processes=True)\n",
    "        client = Client(cluster)\n",
    "        client.amm.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
